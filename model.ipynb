{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR7ri-fV4GTT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import shutil\n",
        "import inspect\n",
        "import logging\n",
        "import joblib\n",
        "from hashlib import sha1\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats, sparse\n",
        "from scipy.stats import chi2_contingency, pearsonr, entropy\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.feature_selection import mutual_info_classif, chi2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    roc_auc_score, f1_score, top_k_accuracy_score\n",
        ")\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "import hdbscan\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoModel, AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup, TrainingArguments, Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sns.set()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFpVUxKDSDhx"
      },
      "source": [
        "##1. Data loading and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkjQaVPSMK1"
      },
      "source": [
        "Apply quotas and sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQzIkMc9RS6G"
      },
      "outputs": [],
      "source": [
        "def make_quota_power_with_ratio(\n",
        "    df, class_col='department_name', target_total=120_000, alpha=0.45, ratio_cap=8.0, k_folds=5, min_per_fold=250, keep_frac=1.0, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    vc = df[class_col].value_counts()\n",
        "    cls = vc.index.to_list()\n",
        "    avail = vc.values.astype(int)\n",
        "\n",
        "    min_need = np.minimum(avail, k_folds * min_per_fold).astype(int)\n",
        "    min_per_class = min_need.copy()\n",
        "\n",
        "    m = int(np.clip(np.median(min_per_class), 1, int(avail.min())))\n",
        "    if m <= 0:\n",
        "        m = int(max(1, avail.min()))\n",
        "\n",
        "    max_cap_scalar = int(ratio_cap * m)\n",
        "    max_per_class = np.minimum(avail, max_cap_scalar)\n",
        "\n",
        "    max_sum = int(max_per_class.sum())\n",
        "    if target_total > max_sum:\n",
        "        target_total = max_sum\n",
        "\n",
        "    min_clip = np.minimum(min_per_class, max_per_class)\n",
        "    min_sum = int(min_clip.sum())\n",
        "    if target_total < min_sum:\n",
        "        target_total = min_sum\n",
        "\n",
        "    w = np.power(np.maximum(avail, 1), alpha)\n",
        "    w = w / w.sum()\n",
        "\n",
        "    raw = w * target_total\n",
        "    quota = np.floor(raw).astype(int)\n",
        "    quota = np.maximum(quota, min_clip)\n",
        "    quota = np.minimum(quota, max_per_class)\n",
        "\n",
        "    deficit = int(target_total - quota.sum())\n",
        "    if deficit > 0:\n",
        "        frac = raw - np.floor(raw)\n",
        "        order = np.argsort(-frac)\n",
        "        for i in order:\n",
        "            if deficit <= 0: break\n",
        "            free = int(max_per_class[i] - quota[i])\n",
        "            if free <= 0: continue\n",
        "            add = min(deficit, free)\n",
        "            quota[i] += add\n",
        "            deficit -= add\n",
        "    elif deficit < 0:\n",
        "        deficit = -deficit\n",
        "        frac = raw - np.floor(raw)\n",
        "        order = np.argsort(frac)\n",
        "        for i in order:\n",
        "            if deficit <= 0: break\n",
        "            can_drop = int(quota[i] - min_clip[i])\n",
        "            if can_drop <= 0: continue\n",
        "            drop = min(deficit, can_drop)\n",
        "            quota[i] -= drop\n",
        "            deficit -= drop\n",
        "\n",
        "    quota = pd.Series(quota, index=cls)\n",
        "\n",
        "    final_min  = int(quota.min())\n",
        "    final_max  = int(quota.max())\n",
        "    final_ratio = final_max / max(1, final_min)\n",
        "\n",
        "    info = dict(\n",
        "        target_total=int(quota.sum()),\n",
        "        min_class=final_min,\n",
        "        max_class=final_max,\n",
        "        ratio=float(final_ratio),\n",
        "        m_ref=int(m),\n",
        "        max_sum=max_sum,\n",
        "        min_sum=min_sum\n",
        "    )\n",
        "    return quota, info\n",
        "\n",
        "quota, info = make_quota_power_with_ratio(\n",
        "    df, class_col='department_name',\n",
        "    target_total=55000,\n",
        "    alpha=0.5,\n",
        "    ratio_cap=15,\n",
        "    k_folds=5,\n",
        "    min_per_fold=200\n",
        ")\n",
        "\n",
        "print(quota.sort_values(ascending=False))\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faxdc6n-RkQU"
      },
      "outputs": [],
      "source": [
        "def strat_sample_by_company(df, class_col='department_name', group_col='company_id',\n",
        "                            quota=None, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    parts = []\n",
        "    for cls, q in quota.items():\n",
        "        sub = df[df[class_col] == cls]\n",
        "        if len(sub) <= q:\n",
        "            parts.append(sub)\n",
        "            continue\n",
        "        gvc = sub[group_col].value_counts()\n",
        "        p = gvc / gvc.sum()\n",
        "        raw = p * q\n",
        "        base = np.floor(raw).astype(int)\n",
        "        rest = q - base.sum()\n",
        "        frac = (raw - base).sort_values(ascending=False)\n",
        "        for comp_id in frac.index[:rest]:\n",
        "            base.loc[comp_id] += 1\n",
        "\n",
        "        for comp_id, k in base.items():\n",
        "            if k <= 0: continue\n",
        "            block = sub[sub[group_col] == comp_id]\n",
        "            if len(block) <= k:\n",
        "                parts.append(block)\n",
        "            else:\n",
        "                parts.append(block.sample(k, random_state=seed))\n",
        "    return pd.concat(parts, axis=0).reset_index(drop=True)\n",
        "\n",
        "df_quota = strat_sample_by_company(df, class_col='department_name',\n",
        "                                  group_col='company_id', quota=quota, seed=42)\n",
        "print(\"Размер после квот:\", len(df_quota))\n",
        "print(df_quota['department_name'].value_counts().sort_values(ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89U8G5r1ygBj"
      },
      "source": [
        "##2. Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yYLMiC6U2ED"
      },
      "source": [
        "###2.1 Feature extraction and Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVJUmxZD5nF2"
      },
      "outputs": [],
      "source": [
        "def analyze_department_distribution(df, target_col='department_name'):\n",
        "    dept_counts = df[target_col].value_counts()\n",
        "    for dept, count in dept_counts.head(10).items():\n",
        "        percentage = count / len(df) * 100\n",
        "        print(f\"  {dept}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    dept_counts.head(15).plot(kind='bar')\n",
        "    plt.title('Top 15 Departments by Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    top_depts = dept_counts.head(8)\n",
        "    others_count = dept_counts.iloc[8:].sum()\n",
        "\n",
        "    if others_count > 0:\n",
        "        plot_data = pd.concat([top_depts, pd.Series({'Others': others_count})])\n",
        "    else:\n",
        "        plot_data = top_depts\n",
        "\n",
        "    plt.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%')\n",
        "    plt.title('Department Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return dept_counts\n",
        "\n",
        "dept_counts = analyze_department_distribution(df, target_col='department_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfEtnmDNfYjs"
      },
      "outputs": [],
      "source": [
        "_whitespace = re.compile(r\"\\s+\")\n",
        "_boiler_patterns = [\n",
        "    r\"^с уважением.*\",\n",
        "    r\"^best regards.*\",\n",
        "    r\"^kind regards.*\",\n",
        "    r\"^отправлено.*\",\n",
        "    r\"^sent from my.*\",\n",
        "    r\"^-{2,}.*\",\n",
        "    r\"^_{2,}.*\",\n",
        "    r\"^\\*{2,}.*\",\n",
        "]\n",
        "\n",
        "def _norm_ws(s: str) -> str:\n",
        "    return _whitespace.sub(\" \", s.strip())\n",
        "\n",
        "def _strip_boilerplate(text: str) -> str:\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip() != \"\"]\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        bad = False\n",
        "        for pat in _boiler_patterns:\n",
        "            if re.match(pat, line, flags=re.I):\n",
        "                bad = True\n",
        "                break\n",
        "        if not bad:\n",
        "            cleaned.append(line)\n",
        "    return \" \".join(cleaned).strip()\n",
        "\n",
        "def merge_subject_details(subject, details, sep=\"[SEP]\", sim_threshold=0.92, max_chars=700):\n",
        "    s = \"\" if pd.isna(subject) else str(subject)\n",
        "    d = \"\" if pd.isna(details) else str(details)\n",
        "\n",
        "    s = _norm_ws(s)\n",
        "    d = _norm_ws(d)\n",
        "\n",
        "    if not d:\n",
        "        return s, 0, 0, len(s)\n",
        "\n",
        "    sim = SequenceMatcher(None, s, d).ratio()\n",
        "    if sim >= sim_threshold or d.startswith(s) or s.startswith(d):\n",
        "        return s, len(s), len(d), len(s)\n",
        "\n",
        "    d2 = d.replace(s, \" \").strip()\n",
        "    d2 = _norm_ws(d2)\n",
        "\n",
        "    d2 = _strip_boilerplate(d2)\n",
        "\n",
        "    if not d2:\n",
        "        return s, len(s), len(d), len(s)\n",
        "\n",
        "    merged = f\"{s} {sep} {d2}\".strip()\n",
        "\n",
        "    if len(merged) > max_chars:\n",
        "        merged = merged[:max_chars].rstrip()\n",
        "\n",
        "    return merged, len(s), len(d), len(merged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPvIi9Bj6d8i"
      },
      "outputs": [],
      "source": [
        "def process_data(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    df = df[~df['subject'].isnull()]\n",
        "    df = df[~df['department_name'].isnull()]\n",
        "    df['company_id'] = df['company_id'].fillna('__unknown__')\n",
        "\n",
        "    df['is_subtask'] = df['parent_uuid'].notnull().astype(int)\n",
        "\n",
        "    merged = df.apply(\n",
        "        lambda r: merge_subject_details(r['subject'], r['details']),\n",
        "        axis=1, result_type='expand'\n",
        "    )\n",
        "    merged.columns = ['text', 'len_subject', 'len_details', 'len_text']\n",
        "\n",
        "    df = pd.concat([df, merged], axis=1)\n",
        "    df = df[df['text'].str.strip() != \"\"]\n",
        "\n",
        "    df = df.drop(columns=['parent_uuid', 'subject', 'details'])\n",
        "\n",
        "    df['has_details'] = (df['len_details'] > 0).astype(int)\n",
        "    df['added_details'] = ((df['len_text'] > df['len_subject']) & (df['has_details'] == 1)).astype(int)\n",
        "\n",
        "    print(\"nulls:\\n\", df.isnull().sum())\n",
        "    return df\n",
        "\n",
        "df_processed = process_data(df)\n",
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeiohhXpUjUJ"
      },
      "source": [
        "Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjHqvdcTgeSk"
      },
      "outputs": [],
      "source": [
        "def normalize_text_preserve_info(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        s = \"\" if pd.isna(s) else str(s)\n",
        "\n",
        "    x = s.strip().lower()\n",
        "    x = x.replace(\"\\u00a0\", \" \")\n",
        "    x = re.sub(r'[\\u200b\\u200c\\u200d\\u2060]', '', x)\n",
        "\n",
        "    x = re.sub(r'(?:https?://|ftp://|www\\.)\\S+', ' <url> ', x, flags=re.I)\n",
        "    x = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', ' <ip> ', x)\n",
        "\n",
        "    x = re.sub(r'\\s+', ' ', x)\n",
        "\n",
        "    return x.strip()\n",
        "\n",
        "df_processed['text_norm'] = df_processed['text'].apply(normalize_text_preserve_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1j-inDvUqV-"
      },
      "source": [
        "###2.2 Noise Filtering (Duplicates, HDBSCAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXxcg1aUVO90"
      },
      "source": [
        "Remove exact duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtTpgnNVlklV"
      },
      "outputs": [],
      "source": [
        "df_processed[\"uid\"] = df_processed[\"text_norm\"].map(lambda s: sha1(s.encode(\"utf-8\")).hexdigest())\n",
        "df_processed = df_processed.sort_values([\"uid\", \"company_id\"]).reset_index(drop=True)\n",
        "df_processed[\"is_exact_dup\"] = df_processed.duplicated(\"uid\", keep=\"first\")\n",
        "\n",
        "df_processed[\"global_drop_reason\"] = None\n",
        "df_processed.loc[df_processed[\"is_exact_dup\"], \"global_drop_reason\"] = \"exact_dup\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcum61ycVU3q"
      },
      "source": [
        "Remove near-duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtU-K2U1lki8"
      },
      "outputs": [],
      "source": [
        "WS_RE = re.compile(r'\\s+')\n",
        "def _prep_for_shingles(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = s.replace('\\u00a0',' ')\n",
        "    s = WS_RE.sub(' ', s)\n",
        "    return s\n",
        "\n",
        "def _char_ngrams(s: str, lo=3, hi=5):\n",
        "    for n in range(lo, hi+1):\n",
        "        for i in range(0, max(0, len(s)-n+1)):\n",
        "            yield s[i:i+n]\n",
        "\n",
        "def _hash64(x: str, seed: int = 0) -> int:\n",
        "    return (hash((seed, x)) & 0xffffffffffffffff)\n",
        "\n",
        "def simhash64(text: str, lo=3, hi=5, n_feat=128):\n",
        "    s = _prep_for_shingles(text)\n",
        "    vec = [0]*64\n",
        "    feats = []\n",
        "    seen = set()\n",
        "    for g in _char_ngrams(s, lo, hi):\n",
        "        if g in seen: continue\n",
        "        seen.add(g)\n",
        "        feats.append(g)\n",
        "        if len(feats) >= n_feat: break\n",
        "\n",
        "    if not feats:\n",
        "        return 0\n",
        "\n",
        "    for g in feats:\n",
        "        h = _hash64(g)\n",
        "        w = 1\n",
        "        for bit in range(64):\n",
        "            if (h >> bit) & 1:\n",
        "                vec[bit] += w\n",
        "            else:\n",
        "                vec[bit] -= w\n",
        "    out = 0\n",
        "    for bit in range(64):\n",
        "        if vec[bit] >= 0:\n",
        "            out |= (1 << bit)\n",
        "    return out\n",
        "\n",
        "def hamming64(a: int, b: int) -> int:\n",
        "    return (a ^ b).bit_count()\n",
        "\n",
        "def drop_near_dups_simhash_keep_first(texts, bands=8, ham_thr=3, n_feat=256):\n",
        "    n = len(texts)\n",
        "    keep = np.ones(n, dtype=bool)\n",
        "\n",
        "    sims = [simhash64(t, lo=3, hi=5, n_feat=n_feat) for t in texts]\n",
        "\n",
        "    band_size = 64 // bands\n",
        "    buckets = [defaultdict(list) for _ in range(bands)]\n",
        "\n",
        "    for i, h in enumerate(sims):\n",
        "        if not keep[i]:\n",
        "            continue\n",
        "        dup = False\n",
        "        for b in range(bands):\n",
        "            shift = b * band_size\n",
        "            mask = (1 << band_size) - 1\n",
        "            key = (h >> shift) & mask\n",
        "            bucket = buckets[b][key]\n",
        "\n",
        "            for j in bucket:\n",
        "                if not keep[j]:\n",
        "                    continue\n",
        "                if hamming64(h, sims[j]) <= ham_thr:\n",
        "                    keep[i] = False\n",
        "                    dup = True\n",
        "                    break\n",
        "            if dup:\n",
        "                break\n",
        "\n",
        "        if keep[i]:\n",
        "            for b in range(bands):\n",
        "                shift = b * band_size\n",
        "                mask = (1 << band_size) - 1\n",
        "                key = (h >> shift) & mask\n",
        "                buckets[b][key].append(i)\n",
        "\n",
        "    return keep\n",
        "\n",
        "\n",
        "mask_candidates = df_processed[\"global_drop_reason\"].isna() & (~df_processed[\"is_exact_dup\"])\n",
        "texts = df_processed.loc[mask_candidates, \"text_norm\"].tolist()\n",
        "\n",
        "keep_local = drop_near_dups_simhash_keep_first(texts, bands=8, ham_thr=3, n_feat=256)\n",
        "\n",
        "ix = df_processed.index[mask_candidates]\n",
        "df_processed.loc[ix[~keep_local], \"global_drop_reason\"] = \"near_dup\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_processed[df_processed[\"global_drop_reason\"].isna()].copy()\n",
        "print(\"Размер до:\", len(df_processed))\n",
        "print(\"Размер после:\", len(df_clean))\n",
        "print(df_processed[\"global_drop_reason\"].value_counts())"
      ],
      "metadata": {
        "id": "-h1aQvci09PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_x1Nc7Xxxk"
      },
      "source": [
        "HDBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqReWJgFX0XU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Используем: {device}\")\n",
        "\n",
        "model_name = \"Zamza/XLM-roberta-large-ftit-emb-lr01\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB8tyx7hX5V9"
      },
      "outputs": [],
      "source": [
        "def get_embeddings_batched(texts: list[str], batch_size: int = 64):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch_texts = texts[i : i + batch_size]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        all_embeddings.append(cls_emb)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "\n",
        "texts_list = df_clean['text_norm'].tolist()\n",
        "embeddings_1024d = get_embeddings_batched(texts_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfGRXdJ2YF5_"
      },
      "outputs": [],
      "source": [
        "N_COMPONENTS = 64\n",
        "\n",
        "pca = PCA(n_components=N_COMPONENTS, random_state=42)\n",
        "embeddings_64d = pca.fit_transform(embeddings_1024d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyiOyktgYRf9"
      },
      "outputs": [],
      "source": [
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=100,\n",
        "    min_samples=15,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    core_dist_n_jobs=-1,\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "labels = clusterer.fit_predict(embeddings_64d)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artifacts = {\n",
        "    'pca': pca,\n",
        "    'clusterer': clusterer,\n",
        "    'params': {\n",
        "        'min_cluster_size': 100,\n",
        "        'min_samples': 15,\n",
        "        'metric': 'euclidean',\n",
        "        'cluster_selection_method': 'eom'\n",
        "    },\n",
        "    'emb_model_name': model_name,\n",
        "}\n",
        "\n",
        "joblib.dump(artifacts, \"hdbscan_artifacts.joblib\", compress=3)"
      ],
      "metadata": {
        "id": "NRIXshCKk2xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4ftRkvYYfzO"
      },
      "outputs": [],
      "source": [
        "df_processed['hdbscan_label'] = labels\n",
        "df_processed['is_noise'] = (labels == -1)\n",
        "\n",
        "noise_percentage = df_processed['is_noise'].mean() * 100\n",
        "n_clusters = len(np.unique(labels)) - 1\n",
        "\n",
        "print(f\"\\nНайдено {n_clusters} кластеров.\")\n",
        "print(f\"'{noise_percentage:.2f}%' данных помечено как 'шум' (label = -1).\")\n",
        "\n",
        "train_clean_hdbscan = df_processed[df_processed['is_noise'] == False].copy()\n",
        "\n",
        "print(f\"\\nРазмер 'старого' train_clean: {len(df_processed)}\")\n",
        "print(f\"Размер НОВОГО 'train_clean_hdbscan': {len(train_clean_hdbscan)}\")\n",
        "print(train_clean_hdbscan['department_name'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv678ApEV6Og"
      },
      "source": [
        "Create splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1OS1wiBgEqL"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_clean_hdbscan[\"label_id\"] = le.fit_transform(train_clean_hdbscan[\"department_name\"])\n",
        "\n",
        "def create_folds(df, class_col=\"label_id\", n_folds=5, random_state=42):\n",
        "    df_out = df.copy()\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    df_out['fold'] = -1\n",
        "\n",
        "    for fold, (_, val_idx) in enumerate(skf.split(df_out, df_out[class_col])):\n",
        "        df_out.iloc[val_idx, df_out.columns.get_loc('fold')] = fold\n",
        "\n",
        "    return df_out\n",
        "\n",
        "train_clean_with_folds = create_folds(\n",
        "    train_clean_hdbscan,\n",
        "    class_col=\"label_id\",\n",
        "    n_folds=5\n",
        ")\n",
        "\n",
        "print(f\"Финальный train_clean с фолдами: {len(train_clean_with_folds)}\")\n",
        "print(train_clean_with_folds['fold'].value_counts())\n",
        "\n",
        "mapping_y = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "for k, v in mapping_y.items():\n",
        "    print(f\"{k} -> {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqzfPNcspHRx"
      },
      "outputs": [],
      "source": [
        "TEXT_COL = \"text_norm\"\n",
        "\n",
        "def token_len(s):\n",
        "    return len(s.split())\n",
        "\n",
        "def char_len(s):\n",
        "    return len(s)\n",
        "\n",
        "def placeholder_ratios(s):\n",
        "    tokens = s.split()\n",
        "    n = len(tokens) if tokens else 1\n",
        "    cnt = Counter(tokens)\n",
        "    return {\n",
        "        \"ratio_URL\":   cnt.get(\"<url>\", 0)/n\n",
        "    }\n",
        "\n",
        "tmp = train_clean_with_folds[[TEXT_COL, \"label_id\", \"department_name\", \"company_id\"]].copy()\n",
        "tmp[\"len_char\"] = tmp[TEXT_COL].map(char_len)\n",
        "tmp[\"len_tok\"]  = tmp[TEXT_COL].map(token_len)\n",
        "\n",
        "ratios = tmp[TEXT_COL].map(placeholder_ratios).apply(pd.Series)\n",
        "tmp = pd.concat([tmp, ratios], axis=1)\n",
        "\n",
        "by_cls = (tmp\n",
        "    .groupby([\"label_id\",\"department_name\"])\n",
        "    .agg(\n",
        "        n=(\"label_id\",\"size\"),\n",
        "        len_char_mean=(\"len_char\",\"mean\"),\n",
        "        len_char_p10=(\"len_char\",lambda s: np.percentile(s,10)),\n",
        "        len_char_p90=(\"len_char\",lambda s: np.percentile(s,90)),\n",
        "        len_tok_mean=(\"len_tok\",\"mean\"),\n",
        "        url_mean=(\"ratio_URL\",\"mean\"),\n",
        "        n_companies=(\"company_id\", lambda s: s.nunique())\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values(\"n\", ascending=False)\n",
        ")\n",
        "\n",
        "print(by_cls.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G15LpEBeQ8sK"
      },
      "source": [
        "###2.3 Meta-Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desc = train_clean[\"len_text\"].describe(percentiles=[0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99])\n",
        "print(desc)\n",
        "\n",
        "plt.hist(train_clean[\"len_text\"], bins=80, range=(0,300))\n",
        "plt.title(\"Распределение len_text (обрезано до 300)\")\n",
        "plt.xlabel(\"len_text\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "per_class = train_clean.groupby(\"label_id\")[\"len_text\"].describe()[[\"mean\",\"min\",\"max\",\"25%\",\"75%\"]]\n",
        "print(per_class)"
      ],
      "metadata": {
        "id": "Sc5raw9H4U_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYRZftkRWfhD"
      },
      "source": [
        "Enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niB1PEtxWGZo"
      },
      "outputs": [],
      "source": [
        "def enrich_short_texts(row):\n",
        "    text = row['text_norm']\n",
        "\n",
        "    if len(text.split()) < 10:\n",
        "        prefix = []\n",
        "\n",
        "        if row['is_subtask'] == 1:\n",
        "            prefix.append(\"дочерняя задача\")\n",
        "        else:\n",
        "            prefix.append(\"новая задача\")\n",
        "\n",
        "        if row['len_text'] < 20:\n",
        "            prefix.append(\"короткая\")\n",
        "        elif row['len_text'] < 50:\n",
        "            prefix.append(\"средняя\")\n",
        "        else:\n",
        "            prefix.append(\"подробная\")\n",
        "\n",
        "        if row['has_details'] == 1:\n",
        "            prefix.append(\"с деталями\")\n",
        "\n",
        "        text = \" \".join(prefix) + \": \" + text\n",
        "\n",
        "    return text\n",
        "\n",
        "train_clean['text_enriched'] = train_clean.apply(enrich_short_texts, axis=1)\n",
        "train_clean[\"len_text\"] = train_clean[\"text_enriched\"].astype(str).str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riLMLUiHgeM1"
      },
      "outputs": [],
      "source": [
        "fold_vals = train_clean['fold'].values\n",
        "splits_pos = []\n",
        "for f in np.sort(np.unique(fold_vals)):\n",
        "    va_pos = np.where(fold_vals == f)[0]\n",
        "    tr_pos = np.where(fold_vals != f)[0]\n",
        "    splits_pos.append((tr_pos, va_pos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXeJlwRmWqH5"
      },
      "source": [
        "Create domain keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MFxhPGljuYz"
      },
      "outputs": [],
      "source": [
        "def mine_domain_keywords(df, text_col, y_col,\n",
        "                         ngram_range=(1,2), min_df=5, max_df=0.9,\n",
        "                         top_k=40, min_docs=10, min_precision=0.55,\n",
        "                         stop_re=None, seed=42):\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    vect = TfidfVectorizer(\n",
        "        analyzer=\"word\", ngram_range=ngram_range,\n",
        "        min_df=min_df, max_df=max_df, lowercase=True, dtype=np.float32\n",
        "    )\n",
        "    X = vect.fit_transform(df[text_col].astype(str).values)\n",
        "    y = df[y_col].to_numpy()\n",
        "    vocab = np.array(vect.get_feature_names_out())\n",
        "\n",
        "    Xbin = X.copy()\n",
        "    Xbin.data[:] = 1.0\n",
        "\n",
        "    classes = np.unique(y)\n",
        "    result = {}\n",
        "\n",
        "    if stop_re is None:\n",
        "        stop_re = re.compile(r\"^(?:\\d+|<\\w+>|ok|ок|спасибо|заявк|просьб|сообщ|письм|звон|номер|дата)$\")\n",
        "\n",
        "    df_total = np.asarray(Xbin.sum(axis=0)).ravel()\n",
        "\n",
        "    for c in classes:\n",
        "        mask_pos = (y == c)\n",
        "        Xp = Xbin[mask_pos]\n",
        "        Xn = Xbin[~mask_pos]\n",
        "\n",
        "        chi, _ = chi2(Xbin, (y == c).astype(int))\n",
        "        order = np.argsort(-chi)  # от большего к меньшему\n",
        "\n",
        "        df_pos = np.asarray(Xp.sum(axis=0)).ravel()\n",
        "        prec = np.divide(df_pos, np.maximum(df_total, 1))\n",
        "\n",
        "        picks = []\n",
        "        for j in order:\n",
        "            term = vocab[j]\n",
        "            if stop_re.search(term):\n",
        "                continue\n",
        "            dpos = int(df_pos[j]); dall = int(df_total[j])\n",
        "            if dpos < min_docs:\n",
        "                continue\n",
        "            if prec[j] < min_precision:\n",
        "                continue\n",
        "            if len(term) <= 2:\n",
        "                continue\n",
        "            picks.append(term)\n",
        "            if len(picks) >= top_k:\n",
        "                break\n",
        "        result[int(c)] = picks\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCWU5e_jj1mO"
      },
      "outputs": [],
      "source": [
        "def disjoint_keywords_by_precision(df, kw_dict, text_col, y_col):\n",
        "    all_terms = sorted({t for lst in kw_dict.values() for t in lst})\n",
        "    term_re = {t: re.compile(rf\"(?<!\\w){re.escape(t)}(?!\\w)\", flags=re.IGNORECASE) for t in all_terms}\n",
        "\n",
        "    stats = {t: {} for t in all_terms}\n",
        "    for c, terms in kw_dict.items():\n",
        "        sub = df[df[y_col] == c][text_col].astype(str)\n",
        "        for t in terms:\n",
        "            cnt = sub.str.contains(term_re[t]).sum()\n",
        "            stats[t][c] = int(cnt)\n",
        "\n",
        "    total = {}\n",
        "    for t in all_terms:\n",
        "        total[t] = sum(stats[t].values())\n",
        "\n",
        "    cleaned = {c: [] for c in kw_dict.keys()}\n",
        "    for t in all_terms:\n",
        "        if total[t] == 0:\n",
        "            continue\n",
        "        best_c = max(stats[t].keys(), key=lambda c: stats[t][c] / max(1, total[t]))\n",
        "        cleaned[best_c].append(t)\n",
        "\n",
        "    for c in cleaned:\n",
        "        cleaned[c] = sorted(cleaned[c])\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wy-4C6Pj_S_"
      },
      "outputs": [],
      "source": [
        "def compile_domain_feature_rules(data, text_col, y_col, id2prefix):\n",
        "    start_time = time.time()\n",
        "\n",
        "    kw_raw = mine_domain_keywords(\n",
        "        data,\n",
        "        text_col=text_col, y_col=y_col,\n",
        "        ngram_range=(1,2), min_df=5, max_df=0.95,\n",
        "        top_k=50, min_docs=12, min_precision=0.6\n",
        "    )\n",
        "\n",
        "    kw_final = disjoint_keywords_by_precision(\n",
        "        data, kw_raw,\n",
        "        text_col=text_col, y_col=y_col\n",
        "    )\n",
        "\n",
        "    compiled_regexes = {}\n",
        "    for c, terms in kw_final.items():\n",
        "        if not terms: continue\n",
        "        name = id2prefix.get(c, f\"class{c}\")\n",
        "        pattern = r\"|\".join([rf\"(?<!\\w){re.escape(t)}(?!\\w)\" for t in terms])\n",
        "        compiled_regexes[name] = re.compile(pattern, flags=re.IGNORECASE)\n",
        "\n",
        "    artifacts = {\n",
        "        'id2prefix': id2prefix,\n",
        "        'compiled_regexes': compiled_regexes\n",
        "    }\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Артефакты (regex-правила) скомпилированы в памяти за {total_time:.2f} сек.\")\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "def apply_domain_feature_rules(text_enriched: str, feature_artifacts: dict):\n",
        "    id2prefix = feature_artifacts['id2prefix']\n",
        "    compiled_regexes = feature_artifacts['compiled_regexes']\n",
        "\n",
        "    domain_features_vector = np.zeros(len(id2prefix) * 2, dtype=np.float32)\n",
        "\n",
        "    idx = 0\n",
        "    for class_id in sorted(id2prefix.keys()):\n",
        "        name = id2prefix[class_id]\n",
        "\n",
        "        if name in compiled_regexes:\n",
        "            re_pat = compiled_regexes[name]\n",
        "\n",
        "            found = 1 if re_pat.search(text_enriched) else 0\n",
        "            count = len(re_pat.findall(text_enriched))\n",
        "\n",
        "            domain_features_vector[idx] = found\n",
        "            domain_features_vector[idx + 1] = count\n",
        "\n",
        "        idx += 2\n",
        "    return domain_features_vector\n",
        "\n",
        "\n",
        "id2prefix = {\n",
        "  0: \"outstaff\", 1: \"accounting\", 2: \"field\", 3: \"office_hw\",\n",
        "  4: \"reminder\", 5: \"servers\", 6: \"sales\",\n",
        "  7: \"pm\", 8: \"service_center\", 9: \"remote_support\"\n",
        "}\n",
        "\n",
        "domain_feature_rules = compile_domain_feature_rules(\n",
        "  train_clean,\n",
        "  text_col=\"text_enriched\",\n",
        "  y_col=\"label_id\",\n",
        "  id2prefix=id2prefix\n",
        ")\n",
        "\n",
        "domain_features = np.array(\n",
        "    [apply_domain_feature_rules(text, domain_feature_rules)\n",
        "     for text in tqdm(train_clean['text_enriched'], desc=\"  Generating domain feats\")]\n",
        ")\n",
        "\n",
        "n_classes = len(id2prefix)\n",
        "feat_names = [f\"{id2prefix[i]}_{t}\" for i in sorted(id2prefix.keys()) for t in (\"keywords\", \"count\")]\n",
        "\n",
        "domain_features_df = pd.DataFrame(domain_features, columns=feat_names, index=train_clean.index)\n",
        "\n",
        "print(\"\\nГотово! Получены переменные 'domain_features' (numpy) и 'domain_features_df' (pandas).\")\n",
        "print(f\"Форма domain_features_df: {domain_features_df.shape}\")\n",
        "print(domain_features_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjemqXurWurV"
      },
      "source": [
        "Create company features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFm5oWPkebRu"
      },
      "outputs": [],
      "source": [
        "def advanced_company_features(df, splits_pos):\n",
        "    comp = df['company_id'].values\n",
        "    y = df['label_id'].values\n",
        "    n_classes = 10\n",
        "    n_samples = len(df)\n",
        "\n",
        "    features = np.zeros((n_samples, n_classes + 3), dtype=np.float32)\n",
        "\n",
        "    global_class_prior = np.bincount(y, minlength=n_classes) / len(y)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(splits_pos, 1):\n",
        "        print(f\"Fold {fold}: processing {len(va_idx)} samples\")\n",
        "\n",
        "        tr_companies = set(comp[tr_idx])\n",
        "\n",
        "        comp_te = {cls: {} for cls in range(n_classes)}\n",
        "        for c in tr_companies:\n",
        "            mask = (comp[tr_idx] == c)\n",
        "            n_samples_c = mask.sum()\n",
        "            if n_samples_c > 0:\n",
        "                for cls in range(n_classes):\n",
        "                    rate = (y[tr_idx][mask] == cls).mean()\n",
        "                    alpha = min(n_samples_c, 10)\n",
        "                    smoothed = (rate * alpha + global_class_prior[cls] * 10) / (alpha + 10)\n",
        "                    comp_te[cls][c] = smoothed\n",
        "\n",
        "        comp_dominant = {}\n",
        "        for c in tr_companies:\n",
        "            mask = (comp[tr_idx] == c)\n",
        "            if mask.sum() > 5:\n",
        "                cls_counts = np.bincount(y[tr_idx][mask], minlength=n_classes)\n",
        "                dominant_ratio = cls_counts.max() / cls_counts.sum()\n",
        "                comp_dominant[c] = dominant_ratio\n",
        "\n",
        "        comp_diversity = {}\n",
        "        for c in tr_companies:\n",
        "            mask = (comp[tr_idx] == c)\n",
        "            if mask.sum() > 1:\n",
        "                cls_counts = np.bincount(y[tr_idx][mask], minlength=n_classes)\n",
        "                probs = cls_counts / cls_counts.sum()\n",
        "                probs = probs[probs > 0]\n",
        "                entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "                comp_diversity[c] = entropy\n",
        "\n",
        "        for idx in va_idx:\n",
        "            c = comp[idx]\n",
        "            is_new = c not in tr_companies\n",
        "\n",
        "            features[idx, -1] = 1.0 if is_new else 0.0\n",
        "\n",
        "            if is_new:\n",
        "                features[idx, :n_classes] = global_class_prior\n",
        "                features[idx, n_classes] = 0.1\n",
        "                features[idx, n_classes + 1] = np.log(n_classes)\n",
        "            else:\n",
        "                for cls in range(n_classes):\n",
        "                    features[idx, cls] = comp_te[cls].get(c, global_class_prior[cls])\n",
        "                features[idx, n_classes] = comp_dominant.get(c, 0.1)\n",
        "                features[idx, n_classes + 1] = comp_diversity.get(c, 0.0)\n",
        "\n",
        "    return features\n",
        "\n",
        "company_feats_advanced = advanced_company_features(train_clean, splits_pos)\n",
        "print(f\"Shape: {company_feats_advanced.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvEg2UwtWxyg"
      },
      "source": [
        "Create extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Qff-866E6z"
      },
      "outputs": [],
      "source": [
        "train_clean[\"len_char\"] = train_clean[\"text_enriched\"].str.len()\n",
        "train_clean[\"len_tok\"]  = train_clean[\"text_enriched\"].str.split().map(len)\n",
        "train_clean[\"log_len_char\"] = np.log1p(train_clean[\"len_char\"])\n",
        "train_clean[\"log_len_tok\"]  = np.log1p(train_clean[\"len_tok\"])\n",
        "train_clean[\"details_combo\"] = train_clean[\"has_details\"] + train_clean[\"added_details\"]\n",
        "train_clean[\"uniq_ratio\"] = train_clean[\"text_enriched\"].map(lambda s: len(set(s.split())) / (len(s.split()) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_check = [\n",
        "    'is_subtask', 'len_subject', 'len_details', 'len_text',\n",
        "    'has_details', 'added_details', 'len_char', 'len_tok',\n",
        "    'log_len_char', 'log_len_tok', 'details_combo', 'uniq_ratio'\n",
        "]\n",
        "\n",
        "X_feat = train_clean[features_to_check].fillna(0)\n",
        "y_feat = train_clean['label_id']\n",
        "\n",
        "mi = mutual_info_classif(X_feat, y_feat, discrete_features='auto', random_state=42)\n",
        "\n",
        "mi_series = pd.Series(mi, index=X_feat.columns).sort_values(ascending=False)\n",
        "print(mi_series)"
      ],
      "metadata": {
        "id": "a3Vycf4UoZYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra = train_clean[[\n",
        "    'is_subtask', 'len_subject', 'len_details', 'len_text',\n",
        "    'has_details', 'added_details', 'len_char', 'len_tok',\n",
        "    'log_len_char', 'log_len_tok', 'details_combo', 'uniq_ratio'\n",
        "]].to_numpy(dtype=np.float32)"
      ],
      "metadata": {
        "id": "emqPz9PTor2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGzfqJZgRCBB"
      },
      "source": [
        "##3. L1-Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huAOFhZvCstT"
      },
      "source": [
        "### 3.1 TF-IDF (char) + LogReg(OOF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJfOWDMfgeIQ"
      },
      "outputs": [],
      "source": [
        "def fit_predict_char_lr_oof(data, splits, max_features):\n",
        "    n = len(data)\n",
        "    C = data['label_id'].nunique()\n",
        "    oof_proba = np.zeros((n, C), dtype=np.float32)\n",
        "    oof_pred  = np.zeros(n, dtype=np.int64)\n",
        "    fold_acc  = []\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(splits, 1):\n",
        "        tr_df = data.iloc[tr_idx]\n",
        "        va_df = data.iloc[va_idx]\n",
        "\n",
        "        vect = TfidfVectorizer(\n",
        "            analyzer='char', ngram_range=(2, 5),\n",
        "            min_df=2, max_features=max_features,\n",
        "            lowercase=False, sublinear_tf=True, dtype=np.float32\n",
        "        )\n",
        "        X_tr = vect.fit_transform(tr_df['text_enriched'])\n",
        "        X_va = vect.transform(va_df['text_enriched'])\n",
        "        y_tr = tr_df['label_id'].values\n",
        "        y_va = va_df['label_id'].values\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            penalty='l2', C=1, solver='saga',\n",
        "            max_iter=3000, n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        proba = clf.predict_proba(X_va)\n",
        "\n",
        "        pred = proba.argmax(axis=1)\n",
        "\n",
        "        va_original_indices = va_df.index.values\n",
        "        oof_proba[va_original_indices] = proba\n",
        "        oof_pred[va_original_indices] = pred\n",
        "\n",
        "        acc = accuracy_score(y_va, pred)\n",
        "        fold_acc.append(acc)\n",
        "\n",
        "        print(f\"[fold {fold}] acc={acc:.4f} train={X_tr.shape} val={X_va.shape}\")\n",
        "\n",
        "    mean, std = float(np.mean(fold_acc)), float(np.std(fold_acc))\n",
        "    print(f\"\\nOOF char accuracy: {mean:.4f} ± {std:.4f}\")\n",
        "    return dict(oof_proba=oof_proba, oof_pred=oof_pred, fold_acc=fold_acc, mean=mean, std=std)\n",
        "\n",
        "res_nat = fit_predict_char_lr_oof(train_clean, splits_pos, max_features=200000)\n",
        "np.save(\"char_lr_oof_proba_enriched_v1_25_hdb.npy\", res_nat['oof_proba'])\n",
        "print(\"\\nreport:\\n\", classification_report(train_clean['label_id'], res_nat['oof_pred'], digits=3))\n",
        "cm = confusion_matrix(train_clean['label_id'], res_nat['oof_pred'])\n",
        "print(f\"\\nConfusion Matrix shape: {cm.shape}, Total: {cm.sum()}\")\n",
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCwKPMD_8pJM"
      },
      "source": [
        "### 3.2 TF-IDF (word) + LogReg(OOF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge-A-F-Z8oy1"
      },
      "outputs": [],
      "source": [
        "def train_word_tfidf(data, splits):\n",
        "    n = len(data)\n",
        "    C = data['label_id'].nunique()\n",
        "\n",
        "    oof = {\n",
        "        'proba': np.zeros((n, C), dtype=np.float32),\n",
        "        'pred': np.zeros(n, dtype=np.int64),\n",
        "        'fold_acc': []\n",
        "    }\n",
        "\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(splits, 1):\n",
        "        tr_df = data.iloc[tr_idx]\n",
        "        va_df = data.iloc[va_idx]\n",
        "\n",
        "        vect = TfidfVectorizer(\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=5,\n",
        "            max_features=150000,\n",
        "            lowercase=True,\n",
        "            sublinear_tf=True,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        X_tr = vect.fit_transform(tr_df['text_enriched'])\n",
        "        X_va = vect.transform(va_df['text_enriched'])\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            C=1.0,\n",
        "            solver='saga',\n",
        "            max_iter=3000,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_tr, tr_df['label_id'].values)\n",
        "\n",
        "        proba = clf.predict_proba(X_va)\n",
        "        pred = proba.argmax(axis=1)\n",
        "\n",
        "        oof['proba'][va_idx] = proba\n",
        "        oof['pred'][va_idx] = pred\n",
        "\n",
        "        acc = accuracy_score(va_df['label_id'].values, pred)\n",
        "        oof['fold_acc'].append(acc)\n",
        "        print(f\"[fold {fold}] acc={acc:.4f} train={X_tr.shape} val={X_va.shape}\")\n",
        "\n",
        "    mean_acc = np.mean(oof['fold_acc'])\n",
        "    print(f\"\\nOOF word accuracy: {mean_acc:.4f} ± {np.std(oof['fold_acc']):.4f}\")\n",
        "    return oof\n",
        "\n",
        "oof_word = train_word_tfidf(train_clean, splits_pos)\n",
        "np.save(\"word_lr_oof_proba_enriched_v1_12_hdbscan.npy\", oof_word['proba'])\n",
        "\n",
        "cm = confusion_matrix(train_clean['label_id'], oof_word['pred'])\n",
        "print(f\"\\nConfusion Matrix shape: {cm.shape}, Total: {cm.sum()}\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axc-Bg1ZXsgQ"
      },
      "source": [
        "### 3.5 Finetune XLM-Roberta and mDeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce63qrgCynYa"
      },
      "outputs": [],
      "source": [
        "def train_xlm_finetuned_oof(data, splits, num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
        "    model_name = 'Zamza/XLM-roberta-large-ftit-emb-lr01'\n",
        "\n",
        "    class TextDataset(Dataset):\n",
        "        def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "            self.texts = texts\n",
        "            self.labels = labels\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.texts)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            text = str(self.texts[idx])\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "    n = len(data)\n",
        "    C = data['label_id'].nunique()\n",
        "    oof_proba = np.zeros((n, C), dtype=np.float32)\n",
        "    fold_acc = []\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    sig = inspect.signature(TrainingArguments)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(splits, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FOLD {fold}/{len(splits)} - Fine-tuning\")\n",
        "        print('='*70)\n",
        "\n",
        "        tr_df = data.loc[tr_idx].copy()\n",
        "        va_df = data.loc[va_idx].copy()\n",
        "\n",
        "        cnt = defaultdict(int)\n",
        "        keep = []\n",
        "        for idx, r in tr_df.iterrows():\n",
        "            key = (r['company_id'], r['label_id'], r['text_enriched'])\n",
        "            if cnt[key] < 5:\n",
        "                keep.append(idx)\n",
        "                cnt[key] += 1\n",
        "        tr_df = tr_df.loc[keep]\n",
        "\n",
        "        train_dataset = TextDataset(\n",
        "            tr_df['text_enriched'].tolist(),\n",
        "            tr_df['label_id'].tolist(),\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        val_dataset = TextDataset(\n",
        "            va_df['text_enriched'].tolist(),\n",
        "            va_df['label_id'].tolist(),\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=C,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        grad_accum = 1\n",
        "\n",
        "        args_kwargs = dict(\n",
        "            output_dir=f\"./mdeberta_fold_fix_{fold}\",\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size * 2,\n",
        "            gradient_accumulation_steps=grad_accum,\n",
        "            num_train_epochs=num_epochs,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=f\"./logs_fix_{fold}\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            report_to=\"none\",\n",
        "            fp16=not (torch.cuda.get_device_capability(0)[0] >= 8),\n",
        "            bf16=(torch.cuda.get_device_capability(0)[0] >= 8),\n",
        "            save_total_limit=1,\n",
        "            label_smoothing_factor=0.1,\n",
        "            learning_rate=learning_rate,\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=2,\n",
        "        )\n",
        "\n",
        "        if \"evaluation_strategy\" in sig.parameters:\n",
        "            args_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            args_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "\n",
        "        if \"save_strategy\" in sig.parameters:\n",
        "            args_kwargs[\"save_strategy\"] = \"epoch\"\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        if \"logging_strategy\" in sig.parameters:\n",
        "            args_kwargs[\"logging_strategy\"] = \"steps\"\n",
        "            args_kwargs[\"logging_steps\"] = 50\n",
        "        else:\n",
        "            args_kwargs[\"logging_steps\"] = 50\n",
        "\n",
        "        if \"lr_scheduler_type\" in sig.parameters:\n",
        "            args_kwargs[\"lr_scheduler_type\"] = \"linear\"\n",
        "\n",
        "        training_args = TrainingArguments(**args_kwargs)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            logits, labels = eval_pred\n",
        "            predictions = np.argmax(logits, axis=-1)\n",
        "            acc = accuracy_score(labels, predictions)\n",
        "            return {'accuracy': acc}\n",
        "\n",
        "        try:\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "            )\n",
        "        except TypeError as e:\n",
        "            print(f\"Ошибка (возможно, 'eval_steps' не поддерживается): {e}\")\n",
        "            print(\"Пробуем запустить без 'eval_steps'...\")\n",
        "\n",
        "            del training_args.eval_steps\n",
        "\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_metrics,\n",
        "            )\n",
        "\n",
        "        print(f\"\\nОбучаем fold {fold}...\")\n",
        "        trainer.train()\n",
        "\n",
        "        print(f\"Предсказываем fold {fold}...\")\n",
        "        predictions = trainer.predict(val_dataset)\n",
        "\n",
        "        logits = predictions.predictions\n",
        "        proba = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "\n",
        "        oof_proba[va_idx] = proba\n",
        "\n",
        "        acc = accuracy_score(va_df['label_id'].values, proba.argmax(axis=1))\n",
        "        fold_acc.append(acc)\n",
        "        print(f\"Fold {fold} accuracy: {acc:.4f}\")\n",
        "\n",
        "\n",
        "        print(f\"Сохраняем лучшую модель фолда {fold} в './best_model_fold_{fold}'\")\n",
        "        trainer.save_model(f'./best_xlmr_fold_{fold}')\n",
        "\n",
        "        del model, trainer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    mean = np.mean(fold_acc)\n",
        "    print(f\"\\n xlm roberta (fine-tuned): {mean:.4f} ± {np.std(fold_acc):.4f}\")\n",
        "\n",
        "    return oof_proba, mean\n",
        "\n",
        "\n",
        "oof_xlm_ft, acc_xlm_ft = train_xlm_finetuned_oof(\n",
        "    train_clean, splits_pos,\n",
        "    num_epochs=10,\n",
        "    batch_size=64,\n",
        "    learning_rate=2e-5\n",
        ")\n",
        "np.save(\"embed_oof_xlm_roberta_finetuned.npy\", oof_xlm_ft)\n",
        "print(f\"Результат: {acc_xlm_ft:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2oHSnHaNUtH"
      },
      "source": [
        "## 4. L2 Model: Stacking\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oof_xlmr = np.load(\"embed_oof_xlm_roberta_finetuned_hdbscan.npy\")  # (N, C)\n",
        "oof_char_25 = np.load(\"char_lr_oof_proba_enriched_v1_25_hdbscan.npy\")  # (N, C)\n",
        "oof_word_12 = np.load(\"word_lr_oof_proba_enriched_v1_12_hdbscan.npy\") # (N, C)\n",
        "\n",
        "y     = train_clean['label_id'].values\n",
        "\n",
        "eps = 1e-6\n",
        "def to_logit(p):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "A = to_logit(oof_xlmr)\n",
        "B = to_logit(oof_char_25)\n",
        "D = to_logit(oof_word_12)\n",
        "\n",
        "X_meta = np.hstack([A,B,D, extra, domain_features, company_feats_advanced]).astype(np.float32)\n",
        "\n",
        "n_samples = len(y)\n",
        "n_classes = int(y.max() + 1)\n",
        "oof_pred = np.zeros(n_samples, dtype=np.int64)\n",
        "oof_proba = np.zeros((n_samples, n_classes), dtype=np.float32)\n",
        "fold_acc = []\n",
        "\n",
        "\n",
        "for fold,(tr_pos,va_pos) in enumerate(splits_pos,1):\n",
        "    X_tr, X_va = X_meta[tr_pos], X_meta[va_pos]\n",
        "    y_tr, y_va = y[tr_pos], y[va_pos]\n",
        "\n",
        "    meta = XGBClassifier(\n",
        "        n_estimators=1500,\n",
        "        learning_rate=0.02,\n",
        "        max_depth=7,\n",
        "        reg_lambda=5.0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist',\n",
        "        device='cuda',\n",
        "        objective='multi:softmax',\n",
        "        num_class=10,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "\n",
        "    meta.fit(X_tr, y_tr,\n",
        "                      eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
        "                      verbose=False)\n",
        "    p = meta.predict_proba(X_va)\n",
        "    oof_pred[va_pos] = np.argmax(p, axis=1)\n",
        "    oof_proba[va_pos] = p\n",
        "\n",
        "    acc = accuracy_score(y_va, oof_pred[va_pos])\n",
        "    fold_acc.append(acc)\n",
        "    print(f\"[fold {fold}] STACK 1: acc={acc:.4f} tr={X_tr.shape} va={X_va.shape}\")\n",
        "\n",
        "\n",
        "mean,std = float(np.mean(fold_acc)), float(np.std(fold_acc))\n",
        "l2_results[\"xgb\"] = mean\n",
        "roc_auc = roc_auc_score(y, oof_proba, multi_class='ovr', average='weighted')\n",
        "print(f\"\\nOOF accuracy STACK: {mean:.4f} ± {std:.4f}\")\n",
        "acc_top3 = top_k_accuracy_score(y, oof_proba, k=3)\n",
        "acc_top5 = top_k_accuracy_score(y, oof_proba, k=5)\n",
        "print(f\"OOF accuracy (Top-3): {acc_top3:.4f}\")\n",
        "print(f\"OOF accuracy (Top-5): {acc_top5:.4f}\")\n",
        "print(f\"OOF ROC AUC (weighted OVR): {roc_auc:.4f}\")\n",
        "print(\"\\nreport:\\n\", classification_report(y, oof_pred, digits=3))\n",
        "cm = confusion_matrix(y, oof_pred, labels=range(int(y.max()+1)))\n",
        "print(\"CM shape:\", cm.shape)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ptV9zaxpo14J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "\n",
        "eps = 1e-6\n",
        "def to_logit(p):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "\n",
        "A = to_logit(np.load(\"embed_oof_xlm_roberta_finetuned_hdbscan.npy\"))\n",
        "B = to_logit(np.load(\"char_lr_oof_proba_enriched_v1_25_hdbscan.npy\"))\n",
        "C = to_logit(np.load(\"word_lr_oof_proba_enriched_v1_12_hdbscan.npy\"))\n",
        "\n",
        "\n",
        "X_meta_FULL = np.hstack([A, B, C, extra, domain_features, company_feats_advanced]).astype(np.float32)\n",
        "y_FULL = train_clean['label_id'].values\n",
        "\n",
        "print(f\"  Форма X_meta_FULL: {X_meta_FULL.shape}\")\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 15.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda',\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': 10,\n",
        "        'early_stopping_rounds': 100\n",
        "    }\n",
        "\n",
        "    fold_acc = []\n",
        "\n",
        "    for fold, (tr_pos, va_pos) in enumerate(splits_pos, 1):\n",
        "        X_tr, X_va = X_meta_FULL[tr_pos], X_meta_FULL[va_pos]\n",
        "        y_tr, y_va = y_FULL[tr_pos], y_FULL[va_pos]\n",
        "\n",
        "        meta = XGBClassifier(**params)\n",
        "\n",
        "        meta.fit(X_tr, y_tr,\n",
        "                 eval_set=[(X_va, y_va)],\n",
        "                 verbose=False)\n",
        "\n",
        "        preds = meta.predict(X_va)\n",
        "        acc = accuracy_score(y_va, preds)\n",
        "        fold_acc.append(acc)\n",
        "\n",
        "    mean_acc = float(np.mean(fold_acc))\n",
        "    return mean_acc\n",
        "\n",
        "print(\"Запуск тюнинга L2-модели (Optuna)\")\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(f\"  Лучший CV Score: {study.best_value:.5f}\")\n",
        "print(\"  Лучшие Параметры:\")\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "wSOlVNnPsNcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oof_xlmr = np.load(\"embed_oof_xlm_roberta_finetuned_hdbscan.npy\")  # (N, C)\n",
        "oof_char_25 = np.load(\"char_lr_oof_proba_enriched_v1_25_hdbscan.npy\")  # (N, C)\n",
        "oof_word_12 = np.load(\"word_lr_oof_proba_enriched_v1_12_hdbscan.npy\")\n",
        "\n",
        "y     = train_clean['label_id'].values\n",
        "\n",
        "eps = 1e-6\n",
        "def to_logit(p):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "A = to_logit(oof_xlmr)\n",
        "B = to_logit(oof_char_25)\n",
        "C = to_logit(oof_word_12)\n",
        "\n",
        "X_meta = np.hstack([A,B,C, extra, domain_features, company_feats_advanced]).astype(np.float32)\n",
        "\n",
        "n_samples = len(y)\n",
        "n_classes = int(y.max() + 1)\n",
        "oof_pred = np.zeros(n_samples, dtype=np.int64)\n",
        "oof_proba = np.zeros((n_samples, n_classes), dtype=np.float32)\n",
        "fold_acc = []\n",
        "history_per_fold = {}\n",
        "\n",
        "params = {\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.01253085083690128,\n",
        "    'max_depth': 10,\n",
        "    'reg_lambda': 3.507499070233551,\n",
        "    'reg_alpha': 1.3946130633958758,\n",
        "    'subsample': 0.7935545348903001,\n",
        "    'colsample_bytree': 0.7853058760189121,\n",
        "    'gamma': 0.8458201681412074,\n",
        "    'min_child_weight': 5,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda',\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 10,\n",
        "    'early_stopping_rounds': 100}\n",
        "\n",
        "for fold,(tr_pos,va_pos) in enumerate(splits_pos,1):\n",
        "    X_tr, X_va = X_meta[tr_pos], X_meta[va_pos]\n",
        "    y_tr, y_va = y[tr_pos], y[va_pos]\n",
        "\n",
        "    meta = XGBClassifier(\n",
        "        **params\n",
        "    )\n",
        "\n",
        "\n",
        "    meta.fit(X_tr, y_tr,\n",
        "                      eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
        "                      verbose=False)\n",
        "    p = meta.predict_proba(X_va)\n",
        "    oof_pred[va_pos] = np.argmax(p, axis=1)\n",
        "    oof_proba[va_pos] = p\n",
        "\n",
        "    evals_res = meta.evals_result()\n",
        "    history_per_fold[f'fold_{fold}'] = evals_res\n",
        "    try:\n",
        "        best_it = meta.get_booster().best_iteration\n",
        "    except Exception:\n",
        "        best_it = getattr(meta, \"best_iteration\", None)\n",
        "\n",
        "    acc = accuracy_score(y_va, oof_pred[va_pos])\n",
        "    print(f\"[fold {fold}] best_it={best_it}, acc={acc:.4f}\")\n",
        "    fold_acc.append(acc)\n",
        "    print(f\"[fold {fold}] STACK : acc={acc:.4f} tr={X_tr.shape} va={X_va.shape}\")\n",
        "\n",
        "\n",
        "mean,std = float(np.mean(fold_acc)), float(np.std(fold_acc))\n",
        "roc_auc = roc_auc_score(y, oof_proba, multi_class='ovr', average='weighted')\n",
        "print(f\"\\nOOF accuracy STACK: {mean:.4f} ± {std:.4f}\")\n",
        "acc_top3 = top_k_accuracy_score(y, oof_proba, k=3)\n",
        "acc_top5 = top_k_accuracy_score(y, oof_proba, k=5)\n",
        "print(f\"OOF accuracy (Top-3): {acc_top3:.4f}\")\n",
        "print(f\"OOF accuracy (Top-5): {acc_top5:.4f}\")\n",
        "print(f\"OOF ROC AUC (weighted OVR): {roc_auc:.4f}\")\n",
        "print(\"\\nreport:\\n\", classification_report(y, oof_pred, digits=3))\n",
        "cm = confusion_matrix(y, oof_pred, labels=range(int(y.max()+1)))\n",
        "print(\"CM shape:\", cm.shape)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "k6QPfpct36f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in history_per_fold.items():\n",
        "    train_mlog = v['validation_0']['mlogloss']\n",
        "    val_mlog = v['validation_1']['mlogloss']\n",
        "    plt.plot(val_mlog, label=f\"{k}_val\")\n",
        "plt.legend();\n",
        "plt.xlabel(\"boost round\");\n",
        "plt.ylabel(\"mlogloss\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4W11Ia18MFiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y9Nv8UqYdeT"
      },
      "source": [
        "##5. Production Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aOwx3iv9iY7"
      },
      "source": [
        "###5.1 Enriching text and creating 'extra' features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncuMUdeoUcCs"
      },
      "outputs": [],
      "source": [
        "def enrich_short_texts(row):\n",
        "    text = row['text_norm']\n",
        "\n",
        "    if len(text.split()) < 10:\n",
        "        prefix = []\n",
        "\n",
        "        if row['is_subtask'] == 1:\n",
        "            prefix.append(\"дочерняя задача\")\n",
        "        else:\n",
        "            prefix.append(\"новая задача\")\n",
        "\n",
        "        if row['len_text'] < 20:\n",
        "            prefix.append(\"короткая\")\n",
        "        elif row['len_text'] < 50:\n",
        "            prefix.append(\"средняя\")\n",
        "        else:\n",
        "            prefix.append(\"подробная\")\n",
        "\n",
        "        if row['has_details'] == 1:\n",
        "            prefix.append(\"с деталями\")\n",
        "\n",
        "        text = \" \".join(prefix) + \": \" + text\n",
        "\n",
        "    return text\n",
        "\n",
        "train_clean['text_enriched'] = train_clean.apply(enrich_short_texts, axis=1)\n",
        "train_clean[\"len_text\"] = train_clean[\"text_enriched\"].astype(str).str.len()\n",
        "train_clean[\"len_char\"] = train_clean[\"text_enriched\"].str.len()\n",
        "train_clean[\"len_tok\"]  = train_clean[\"text_enriched\"].str.split().map(len)\n",
        "train_clean[\"log_len_char\"] = np.log1p(train_clean[\"len_char\"])\n",
        "train_clean[\"log_len_tok\"]  = np.log1p(train_clean[\"len_tok\"])\n",
        "train_clean[\"details_combo\"] = train_clean[\"has_details\"] + train_clean[\"added_details\"]\n",
        "train_clean[\"uniq_ratio\"] = train_clean[\"text_enriched\"].map(lambda s: len(set(s.split())) / (len(s.split()) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ4yqCGk9Syx"
      },
      "outputs": [],
      "source": [
        "extra = train_clean[[\n",
        "    'is_subtask', 'len_subject', 'len_details', 'len_text',\n",
        "    'has_details', 'added_details', 'len_char', 'len_tok',\n",
        "    'log_len_char', 'log_len_tok', 'details_combo', 'uniq_ratio'\n",
        "]].to_numpy(dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw5m4omM9pF-"
      },
      "source": [
        "### 5.2 Creating and saving domain features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5pexPgfUepO"
      },
      "outputs": [],
      "source": [
        "def mine_domain_keywords(df, text_col, y_col,\n",
        "                         ngram_range=(1,2), min_df=5, max_df=0.9,\n",
        "                         top_k=40, min_docs=10, min_precision=0.55,\n",
        "                         stop_re=None, seed=42):\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    vect = TfidfVectorizer(\n",
        "        analyzer=\"word\", ngram_range=ngram_range,\n",
        "        min_df=min_df, max_df=max_df, lowercase=True, dtype=np.float32\n",
        "    )\n",
        "    X = vect.fit_transform(df[text_col].astype(str).values)\n",
        "    y = df[y_col].to_numpy()\n",
        "    vocab = np.array(vect.get_feature_names_out())\n",
        "\n",
        "    Xbin = X.copy()\n",
        "    Xbin.data[:] = 1.0\n",
        "\n",
        "    classes = np.unique(y)\n",
        "    result = {}\n",
        "\n",
        "    if stop_re is None:\n",
        "        stop_re = re.compile(r\"^(?:\\d+|<\\w+>|ok|ок|спасибо|заявк|просьб|сообщ|письм|звон|номер|дата)$\")\n",
        "\n",
        "    df_total = np.asarray(Xbin.sum(axis=0)).ravel()\n",
        "\n",
        "    for c in classes:\n",
        "        mask_pos = (y == c)\n",
        "        Xp = Xbin[mask_pos]\n",
        "        Xn = Xbin[~mask_pos]\n",
        "\n",
        "        chi, _ = chi2(Xbin, (y == c).astype(int))\n",
        "        order = np.argsort(-chi)\n",
        "\n",
        "        df_pos = np.asarray(Xp.sum(axis=0)).ravel()\n",
        "        prec = np.divide(df_pos, np.maximum(df_total, 1))\n",
        "\n",
        "        picks = []\n",
        "        for j in order:\n",
        "            term = vocab[j]\n",
        "            if stop_re.search(term):\n",
        "                continue\n",
        "            dpos = int(df_pos[j]); dall = int(df_total[j])\n",
        "            if dpos < min_docs:\n",
        "                continue\n",
        "            if prec[j] < min_precision:\n",
        "                continue\n",
        "            if len(term) <= 2:\n",
        "                continue\n",
        "            picks.append(term)\n",
        "            if len(picks) >= top_k:\n",
        "                break\n",
        "        result[int(c)] = picks\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fW3R2sCUe99"
      },
      "outputs": [],
      "source": [
        "def disjoint_keywords_by_precision(df, kw_dict, text_col, y_col):\n",
        "    all_terms = sorted({t for lst in kw_dict.values() for t in lst})\n",
        "    term_re = {t: re.compile(rf\"(?<!\\w){re.escape(t)}(?!\\w)\", flags=re.IGNORECASE) for t in all_terms}\n",
        "\n",
        "    stats = {t: {} for t in all_terms}\n",
        "    for c, terms in kw_dict.items():\n",
        "        sub = df[df[y_col] == c][text_col].astype(str)\n",
        "        for t in terms:\n",
        "            cnt = sub.str.contains(term_re[t]).sum()\n",
        "            stats[t][c] = int(cnt)\n",
        "\n",
        "    total = {}\n",
        "    for t in all_terms:\n",
        "        total[t] = sum(stats[t].values())\n",
        "\n",
        "    cleaned = {c: [] for c in kw_dict.keys()}\n",
        "    for t in all_terms:\n",
        "        if total[t] == 0:\n",
        "            continue\n",
        "        best_c = max(stats[t].keys(), key=lambda c: stats[t][c] / max(1, total[t]))\n",
        "        cleaned[best_c].append(t)\n",
        "\n",
        "    for c in cleaned:\n",
        "        cleaned[c] = sorted(cleaned[c])\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smWu-lSuU17l"
      },
      "outputs": [],
      "source": [
        "def build_prod_domain_features(data, text_col, y_col, id2prefix, save_path=\"prod_domain_features.joblib\"):\n",
        "    start_time = time.time()\n",
        "\n",
        "    kw_raw = mine_domain_keywords(\n",
        "        data,\n",
        "        text_col=text_col, y_col=y_col,\n",
        "        ngram_range=(1,2), min_df=5, max_df=0.95,\n",
        "        top_k=50, min_docs=12, min_precision=0.6\n",
        "    )\n",
        "\n",
        "    kw_final = disjoint_keywords_by_precision(\n",
        "        data, kw_raw,\n",
        "        text_col=text_col, y_col=y_col\n",
        "    )\n",
        "\n",
        "    compiled_regexes = {}\n",
        "    for c, terms in kw_final.items():\n",
        "        if not terms: continue\n",
        "        name = id2prefix.get(c, f\"class{c}\")\n",
        "        pattern = r\"|\".join([rf\"(?<!\\w){re.escape(t)}(?!\\w)\" for t in terms])\n",
        "        compiled_regexes[name] = re.compile(pattern, flags=re.IGNORECASE)\n",
        "\n",
        "    artifacts = {\n",
        "        'id2prefix': id2prefix,\n",
        "        'compiled_regexes': compiled_regexes\n",
        "    }\n",
        "\n",
        "    joblib.dump(artifacts, save_path)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Файл готов за {total_time:.2f} сек.\")\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "def generate_domain_features_for_inference(text_enriched: str, feature_maps: dict):\n",
        "    id2prefix = feature_maps['id2prefix']\n",
        "    compiled_regexes = feature_maps['compiled_regexes']\n",
        "\n",
        "    domain_features = np.zeros(len(id2prefix) * 2, dtype=np.float32)\n",
        "\n",
        "    idx = 0\n",
        "    for class_id in sorted(id2prefix.keys()):\n",
        "        name = id2prefix[class_id]\n",
        "\n",
        "        if name in compiled_regexes:\n",
        "            re_pat = compiled_regexes[name]\n",
        "\n",
        "            found = 1 if re_pat.search(text_enriched) else 0\n",
        "\n",
        "            count = len(re_pat.findall(text_enriched))\n",
        "\n",
        "            domain_features[idx] = found\n",
        "            domain_features[idx + 1] = count\n",
        "\n",
        "        idx += 2\n",
        "\n",
        "    return domain_features\n",
        "\n",
        "\n",
        "id2prefix = {\n",
        "  0: \"outstaff\", 1: \"accounting\", 2: \"field\", 3: \"office_hw\",\n",
        "  4: \"reminder\", 5: \"servers\", 6: \"sales\",\n",
        "  7: \"pm\", 8: \"service_center\", 9: \"remote_support\"\n",
        "}\n",
        "\n",
        "prod_domain_feature_maps = build_prod_domain_features(\n",
        "  train_clean,\n",
        "  text_col=\"text_enriched\",\n",
        "  y_col=\"label_id\",\n",
        "  id2prefix=id2prefix,\n",
        "  save_path=\"prod_domain_features.joblib\"\n",
        ")\n",
        "\n",
        "domain_features = np.array(\n",
        "    [generate_domain_features_for_inference(text, prod_domain_feature_maps)\n",
        "     for text in tqdm(train_clean['text_enriched'], desc=\"  Generating domain feats\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiyUywai9vjM"
      },
      "source": [
        "###5.3 Creating and saving company features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sER6lWu92qyB"
      },
      "outputs": [],
      "source": [
        "def build_prod_company_features(data, save_path=\"prod_company_features.joblib\"):\n",
        "    start_time = time.time()\n",
        "\n",
        "    comp = data['company_id'].values\n",
        "    y = data['label_id'].values\n",
        "\n",
        "    n_classes = 10\n",
        "    global_class_prior = np.bincount(y, minlength=n_classes) / len(y)\n",
        "\n",
        "    comp_te_per_class_map = {cls: {} for cls in range(n_classes)}\n",
        "    for c in np.unique(comp):\n",
        "        mask = (comp == c)\n",
        "        n_samples_c = mask.sum()\n",
        "        for cls in range(n_classes):\n",
        "            if n_samples_c > 0:\n",
        "                rate = (y[mask] == cls).mean()\n",
        "                alpha = min(n_samples_c, 10)\n",
        "                smoothed = (rate * alpha + global_class_prior[cls] * 10) / (alpha + 10)\n",
        "                comp_te_per_class_map[cls][c] = float(smoothed)\n",
        "            else:\n",
        "                comp_te_per_class_map[cls][c] = float(global_class_prior[cls])\n",
        "\n",
        "    comp_main_cls_map = {}\n",
        "    for c in np.unique(comp):\n",
        "        mask = (comp == c)\n",
        "        n_tasks = mask.sum()\n",
        "        if n_tasks > 5:\n",
        "            cls_counts = np.bincount(y[mask], minlength=10)\n",
        "            dominant = cls_counts.argmax()\n",
        "            comp_main_cls_map[c] = cls_counts[dominant] / n_tasks\n",
        "        else:\n",
        "            comp_main_cls_map[c] = 0.1\n",
        "\n",
        "    comp_div_map = {}\n",
        "    for c in np.unique(comp):\n",
        "        mask = (comp == c)\n",
        "        if mask.sum() > 1:\n",
        "            cls_counts = np.bincount(y[mask], minlength=10)\n",
        "            probs = cls_counts / cls_counts.sum()\n",
        "            probs = probs[probs > 0]\n",
        "            entropy = -np.sum(probs * np.log(probs))\n",
        "            comp_div_map[c] = entropy\n",
        "        else:\n",
        "            comp_div_map[c] = 0.0\n",
        "\n",
        "    known_companies = set(np.unique(comp).tolist())\n",
        "\n",
        "    artifacts = {\n",
        "        'te_per_class': comp_te_per_class_map,\n",
        "        'dominant_class': comp_main_cls_map,\n",
        "        'diversity': comp_div_map,\n",
        "        'priors': {\n",
        "            'te_per_class': 0.1,\n",
        "            'dominant_class': 0.1,\n",
        "            'diversity': 0.0\n",
        "        },\n",
        "        'known_companies': known_companies,\n",
        "    }\n",
        "\n",
        "    joblib.dump(artifacts, save_path)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Файл готов за {total_time:.2f} сек.\")\n",
        "\n",
        "    return artifacts\n",
        "\n",
        "def generate_company_features_for_inference(company_id: str, feature_maps: dict):\n",
        "    n_classes = 10\n",
        "    te_features = np.zeros(10, dtype=np.float32)\n",
        "    for cls in range(10):\n",
        "        te_features[cls] = feature_maps['te_per_class'][cls].get(\n",
        "            company_id,\n",
        "            feature_maps['priors']['te_per_class']\n",
        "        )\n",
        "\n",
        "    dominant_feature = feature_maps['dominant_class'].get(\n",
        "        company_id,\n",
        "        feature_maps['priors']['dominant_class']\n",
        "    )\n",
        "\n",
        "    diversity_feature = feature_maps['diversity'].get(\n",
        "        company_id,\n",
        "        feature_maps['priors']['diversity']\n",
        "    )\n",
        "\n",
        "    known_companies = feature_maps.get('known_companies', set())\n",
        "    is_new = np.float32(0.0 if company_id in known_companies else 1.0)\n",
        "\n",
        "    return np.hstack([\n",
        "        te_features,\n",
        "        dominant_feature,\n",
        "        diversity_feature,\n",
        "        is_new\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "\n",
        "feature_maps = build_prod_company_features(train_clean, \"prod_company_features.joblib\")\n",
        "\n",
        "company_features = np.array(\n",
        "    [generate_company_features_for_inference(cid, feature_maps)\n",
        "    for cid in tqdm(train_clean['company_id'], desc=\"  Generating company feats\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh69a6WS-BH3"
      },
      "source": [
        "###5.4 Training and saving L1 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmAyf4Mt-IO4"
      },
      "source": [
        "#### 5.4.1 TF-IDF + LogReg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOsSXedZVQLz"
      },
      "outputs": [],
      "source": [
        "def build_prod_tfidf_lr_model(data, save_prefix, analyzer, ngram_range, max_features, C):\n",
        "  X_text = data['text_enriched']\n",
        "  y = data['label_id'].values\n",
        "\n",
        "  vect = TfidfVectorizer(\n",
        "      analyzer=analyzer,\n",
        "      ngram_range=ngram_range,\n",
        "      min_df=2,\n",
        "      max_features=max_features,\n",
        "      lowercase=False,\n",
        "      sublinear_tf=True,\n",
        "      dtype=np.float32\n",
        "  )\n",
        "\n",
        "  start_time = time.time()\n",
        "  X_tf = vect.fit_transform(X_text)\n",
        "  print(f\"TF-IDF обучена за {time.time() - start_time:.2f} сек. Форма: {X_tf.shape}\")\n",
        "\n",
        "  clf = LogisticRegression(\n",
        "      penalty='l2',\n",
        "      C=C,\n",
        "      solver='saga',\n",
        "      max_iter=3000,\n",
        "      n_jobs=-1,\n",
        "      random_state=42\n",
        "  )\n",
        "\n",
        "  start_time = time.time()\n",
        "  clf.fit(X_tf, y)\n",
        "  print(f\"LogReg обучена за {time.time() - start_time:.2f} сек.\")\n",
        "\n",
        "  vect_path = f\"{save_prefix}_vect.joblib\"\n",
        "  clf_path = f\"{save_prefix}_clf.joblib\"\n",
        "\n",
        "  joblib.dump(vect, vect_path)\n",
        "  joblib.dump(clf, clf_path)\n",
        "\n",
        "  print(f\"  Артефакты сохранены:\\n    1. {vect_path}\\n    2. {clf_path}\")\n",
        "\n",
        "  return vect, clf\n",
        "\n",
        "if not isinstance(train_clean.index, pd.RangeIndex):\n",
        "  train_clean = train_clean.reset_index(drop=True)\n",
        "\n",
        "vect_b, clf_b = build_prod_tfidf_lr_model(\n",
        "        data=train_clean,\n",
        "        save_prefix=\"prod_char_lr_25\",\n",
        "        analyzer='char',\n",
        "        ngram_range=(2, 5),\n",
        "        max_features=200000,\n",
        "        C=1.0\n",
        "    )\n",
        "\n",
        "vect_e, clf_e = build_prod_tfidf_lr_model(\n",
        "        data=train_clean,\n",
        "        save_prefix=\"prod_word_lr_12\",\n",
        "        analyzer='word',\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=150000,\n",
        "        C=1.0\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSxfu1KyHwyt"
      },
      "source": [
        "####5.4.2 XLM-Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "proQ76EwIrs7"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def build_prod_transformer_model(data, model_name, save_path, num_epochs=5, batch_size=64, learning_rate=2e-5):\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_text = data['text_enriched'].tolist()\n",
        "    y_labels = data['label_id'].tolist()\n",
        "    C = data['label_id'].nunique()\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_text, y_labels, test_size=0.10, random_state=42, stratify=y_labels\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=C,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
        "    val_dataset = TextDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        preds = np.argmax(pred.predictions, axis=1)\n",
        "        labels = pred.label_ids\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        return {\"accuracy\": float(acc)}\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./temp_logs_{save_path}\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size * 2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"none\",\n",
        "        fp16=not (torch.cuda.get_device_capability(0)[0] >= 8),\n",
        "        bf16=(torch.cuda.get_device_capability(0)[0] >= 8),\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=100,\n",
        "        dataloader_num_workers=2,\n",
        "        logging_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"  Обучение завершено. Сохраняем модель в {save_path}...\")\n",
        "    trainer.save_model(save_path)\n",
        "\n",
        "    print(f\"  Сохраняем ТОКЕНИЗАТОР в {save_path}...\")\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    del model, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\" Завершено за {total_time:.2f} сек.\")\n",
        "\n",
        "\n",
        "build_prod_transformer_model(\n",
        "    data=train_clean,\n",
        "    model_name='Zamza/XLM-roberta-large-ftit-emb-lr01',\n",
        "    save_path='prod_xlmr_finetuned_hdbscan',\n",
        "    num_epochs=8,\n",
        "    batch_size=64,\n",
        "    learning_rate=2e-5\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewlK6na1dw9"
      },
      "source": [
        "###5.5 Training and saving L2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQMToc14KhlS"
      },
      "outputs": [],
      "source": [
        "y_FULL = train_clean['label_id'].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "\n",
        "oof_xlmr = np.load(\"embed_oof_xlm_roberta_finetuned_hdbscan.npy\")  # (N, C)\n",
        "oof_char_25 = np.load(\"char_lr_oof_proba_enriched_v1_25_hdbscan.npy\")  # (N, C)\n",
        "oof_word_12 = np.load(\"word_lr_oof_proba_enriched_v1_12_hdbscan.npy\") # (N, C)\n",
        "\n",
        "def to_logit(p):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "A = to_logit(oof_xlmr)\n",
        "B = to_logit(oof_char_25)\n",
        "C = to_logit(oof_word_12)\n",
        "\n",
        "X_meta_FULL = np.hstack([A,B,C, extra, domain_features, company_features]).astype(np.float32)\n",
        "\n",
        "print(f\"Финальная форма X_meta_FULL: {X_meta_FULL.shape}\")\n",
        "print(f\"Финальная форма y_FULL: {y_FULL.shape}\")\n",
        "\n",
        "params = {\n",
        "    'n_estimators': 800,\n",
        "    'learning_rate': 0.01253085083690128,\n",
        "    'max_depth': 10,\n",
        "    'reg_lambda': 3.507499070233551,\n",
        "    'reg_alpha': 1.3946130633958758,\n",
        "    'subsample': 0.7935545348903001,\n",
        "    'colsample_bytree': 0.7853058760189121,\n",
        "    'gamma': 0.8458201681412074,\n",
        "    'min_child_weight': 5,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda',\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 10}\n",
        "\n",
        "final_stacker = XGBClassifier(\n",
        "    **params\n",
        ")\n",
        "\n",
        "final_stacker.fit(X_meta_FULL, y_FULL, verbose=100)\n",
        "\n",
        "save_path_l2 = \"prod_stacker_L2.xgb\"\n",
        "final_stacker.save_model(save_path_l2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmCe5tIQldkj"
      },
      "source": [
        "##6. Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwdSZE9LtAW3"
      },
      "outputs": [],
      "source": [
        "def load_prod_features(load_path):\n",
        "    if not os.path.exists(load_path):\n",
        "        print(f\"ОШИБКА: Файл артефактов L2-фич не найден: {load_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Загрузка артефактов L2-фич из {load_path}...\")\n",
        "    artifacts = joblib.load(load_path)\n",
        "    return artifacts\n",
        "\n",
        "def load_all_prod_artifacts():\n",
        "    artifacts = {}\n",
        "\n",
        "    # A (XLM-R)\n",
        "    artifacts['tok_A'] = AutoTokenizer.from_pretrained(\"Zamza/XLM-roberta-large-ftit-emb-lr01\")\n",
        "    artifacts['clf_A'] = AutoModelForSequenceClassification.from_pretrained('./prod_xlmr_finetuned_hdbscan').to(device).eval()\n",
        "    # B (char_lr_25)\n",
        "    artifacts['vect_B'] = joblib.load(\"prod_char_lr_25_vect.joblib\")\n",
        "    artifacts['clf_B']  = joblib.load(\"prod_char_lr_25_clf.joblib\")\n",
        "    # C (word_lr_12)\n",
        "    artifacts['vect_C'] = joblib.load(\"prod_word_lr_12_vect.joblib\")\n",
        "    artifacts['clf_C']  = joblib.load(\"prod_word_lr_12_clf.joblib\")\n",
        "\n",
        "    # L2 (Mappers)\n",
        "    artifacts['company_feature_maps'] = load_prod_features(\"prod_company_features.joblib\")\n",
        "    artifacts['domain_feature_maps']  = load_prod_features(\"prod_domain_features.joblib\")\n",
        "\n",
        "    # L2 (XGBoost)\n",
        "    artifacts['stacker_L2'] = XGBClassifier()\n",
        "    artifacts['stacker_L2'].load_model(\"prod_stacker_L2.xgb\")\n",
        "\n",
        "    artifacts['hdbscan'] = joblib.load(\"hdbscan_artifacts.joblib\")\n",
        "\n",
        "    print(\"Все Артифакты загружены успешно!\")\n",
        "\n",
        "    return artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3UsM--6jQh_"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "_whitespace = re.compile(r\"\\s+\")\n",
        "_boiler_patterns = [\n",
        "    r\"^с уважением.*\",\n",
        "    r\"^best regards.*\",\n",
        "    r\"^kind regards.*\",\n",
        "    r\"^отправлено.*\",\n",
        "    r\"^sent from my.*\",\n",
        "    r\"^-{2,}.*\",\n",
        "    r\"^_{2,}.*\",\n",
        "    r\"^\\*{2,}.*\",\n",
        "]\n",
        "\n",
        "def _norm_ws(s: str) -> str:\n",
        "    return _whitespace.sub(\" \", s.strip())\n",
        "\n",
        "def _strip_boilerplate(text: str) -> str:\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip() != \"\"]\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        bad = False\n",
        "        for pat in _boiler_patterns:\n",
        "            if re.match(pat, line, flags=re.I): bad = True; break\n",
        "        if not bad: cleaned.append(line)\n",
        "    return \" \".join(cleaned).strip()\n",
        "\n",
        "def merge_subject_details(subject, details, sep=\"[SEP]\", sim_threshold=0.92, max_chars=700):\n",
        "    s = \"\" if pd.isna(subject) else str(subject)\n",
        "    d = \"\" if pd.isna(details) else str(details)\n",
        "    s = _norm_ws(s)\n",
        "    d = _norm_ws(d)\n",
        "\n",
        "    if not d:\n",
        "        return s, 0, 0, len(s)\n",
        "\n",
        "    sim = SequenceMatcher(None, s, d).ratio()\n",
        "    if sim >= sim_threshold or d.startswith(s) or s.startswith(d):\n",
        "        return s, len(s), len(d), len(s)\n",
        "\n",
        "    d2 = d.replace(s, \" \").strip()\n",
        "    d2 = _norm_ws(d2)\n",
        "\n",
        "    d2 = _strip_boilerplate(d2)\n",
        "\n",
        "    if not d2:\n",
        "        return s, len(s), len(d), len(s)\n",
        "\n",
        "    merged = f\"{s} {sep} {d2}\".strip()\n",
        "\n",
        "    if len(merged) > max_chars:\n",
        "        merged = merged[:max_chars].rstrip()\n",
        "\n",
        "    return merged, len(s), len(d), len(merged)\n",
        "\n",
        "def normalize_text_preserve_info(s: str):\n",
        "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
        "        return \"\"\n",
        "    x = s.strip().lower()\n",
        "    x = x.replace(\"\\\\u00a0\", \" \")\n",
        "    x = re.sub(r'[\\\\u200b\\\\u200c\\\\u200d\\\\u2060]', '', x)\n",
        "    x = re.sub(r'(?:https?://|ftp://|www\\\\.)\\\\S+', ' <url> ', x, flags=re.I)\n",
        "    x = re.sub(r'\\\\b(?:\\\\d{1,3}\\\\.){3}\\\\d{1,3}\\\\b', ' <ip> ', x)\n",
        "    x = re.sub(r'\\\\s+', ' ', x)\n",
        "    return x.strip()\n",
        "\n",
        "def enrich_short_texts(text_norm, is_subtask, len_text, has_details):\n",
        "    text = text_norm\n",
        "    if len(text.split()) < 10:\n",
        "        prefix = []\n",
        "        prefix.append(\"дочерняя задача\" if is_subtask == 1 else \"новая задача\")\n",
        "        if len_text < 20: prefix.append(\"короткая\")\n",
        "        elif len_text < 50: prefix.append(\"средняя\")\n",
        "        else: prefix.append(\"подробная\")\n",
        "        if has_details == 1: prefix.append(\"с деталями\")\n",
        "        text = \" \".join(prefix) + \": \" + text\n",
        "    return text\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p) - np.log1p(-p)\n",
        "\n",
        "def generate_company_features_for_inference(company_id: str, feature_maps: dict):\n",
        "    te_features = np.zeros(10, dtype=np.float32)\n",
        "    for cls in range(10):\n",
        "        te_features[cls] = feature_maps['te_per_class'][cls].get(\n",
        "            company_id, feature_maps['priors']['te_per_class']\n",
        "        )\n",
        "    dominant_feature = feature_maps['dominant_class'].get(\n",
        "        company_id, feature_maps['priors']['dominant_class']\n",
        "    )\n",
        "    diversity_feature = feature_maps['diversity'].get(\n",
        "        company_id, feature_maps['priors']['diversity']\n",
        "    )\n",
        "\n",
        "    known_companies = feature_maps.get('known_companies', set())\n",
        "    is_new = np.float32(0.0 if company_id in known_companies else 1.0)\n",
        "    return np.hstack([\n",
        "        te_features, dominant_feature, diversity_feature, is_new\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "def generate_domain_features_for_inference(text_enriched: str, feature_maps: dict):\n",
        "    id2prefix = feature_maps['id2prefix']\n",
        "    compiled_regexes = feature_maps['compiled_regexes']\n",
        "    domain_features = np.zeros(len(id2prefix) * 2, dtype=np.float32)\n",
        "    idx = 0\n",
        "    for class_id in sorted(id2prefix.keys()):\n",
        "        name = id2prefix[class_id]\n",
        "        if name in compiled_regexes:\n",
        "            re_pat = compiled_regexes[name]\n",
        "            found = 1 if re_pat.search(text_enriched) else 0\n",
        "            count = len(re_pat.findall(text_enriched))\n",
        "            domain_features[idx] = found\n",
        "            domain_features[idx + 1] = count\n",
        "        idx += 2\n",
        "    return domain_features.astype(np.float32)\n",
        "\n",
        "\n",
        "def generate_l1_features(text_enriched: str, artifacts: dict):\n",
        "    # 1. A (XLM-R)\n",
        "    inputs_A = artifacts['tok_A'](text_enriched, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
        "    inputs_A = {k: v.to(device) for k,v in inputs_A.items()}\n",
        "    with torch.no_grad():\n",
        "        logits_A = artifacts['clf_A'](**inputs_A).logits\n",
        "        proba_A = torch.softmax(logits_A, dim=1).cpu().numpy()[0]\n",
        "    # 2. B (char_lr_25)\n",
        "    X_tf_B = artifacts['vect_B'].transform([text_enriched])\n",
        "    proba_B = artifacts['clf_B'].predict_proba(X_tf_B)[0]\n",
        "\n",
        "    # 3. C (word_lr_12)\n",
        "    X_tf_C = artifacts['vect_C'].transform([text_enriched])\n",
        "    proba_C = artifacts['clf_C'].predict_proba(X_tf_C)[0]\n",
        "\n",
        "    # L1\n",
        "    l1_features = np.hstack([\n",
        "        to_logit(proba_A),\n",
        "        to_logit(proba_B),\n",
        "        to_logit(proba_C)\n",
        "    ])\n",
        "    return l1_features.astype(np.float32)\n",
        "\n",
        "def generate_extra_features(text_enriched: str, has_details: int, added_details: int, is_subtask: int, len_subject: int, len_details: int):\n",
        "    is_missing = (text_enriched is None) or (isinstance(text_enriched, float) and np.isnan(text_enriched))\n",
        "\n",
        "    s_str = str(text_enriched)\n",
        "    len_text = len(s_str)\n",
        "\n",
        "    len_char = np.nan if is_missing else len(s_str)\n",
        "\n",
        "    tokens = [] if is_missing else s_str.split()\n",
        "    len_tok = len(tokens)\n",
        "\n",
        "    uniq_ratio = len(set(tokens)) / (len_tok + 1)\n",
        "\n",
        "    log_len_char = np.nan if is_missing else np.log1p(len_char)\n",
        "    log_len_tok = np.log1p(len_tok)\n",
        "\n",
        "\n",
        "    len_text = len(text_enriched)\n",
        "    tokens = text_enriched.split()\n",
        "    len_tok = len(tokens)\n",
        "    uniq_ratio = len(set(tokens)) / (len_tok + 1)\n",
        "\n",
        "    details_combo = int(has_details) + int(added_details)\n",
        "\n",
        "    basic_features = np.array([\n",
        "        is_subtask,\n",
        "        len_subject,\n",
        "        len_details,\n",
        "        len_text,\n",
        "        has_details,\n",
        "        added_details,\n",
        "        len_char if not np.isnan(len_char) else np.nan,\n",
        "        len_tok,\n",
        "        log_len_char if not np.isnan(log_len_char) else np.nan,\n",
        "        log_len_tok,\n",
        "        details_combo,\n",
        "        uniq_ratio\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "    return basic_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9QSUwiEm-IG"
      },
      "outputs": [],
      "source": [
        "def predict_one_pipeline(\n",
        "    raw_subject: str,\n",
        "    raw_details: str,\n",
        "    raw_parent_uuid: str,\n",
        "    raw_company_id: str,\n",
        "    artifacts_bundle: dict\n",
        "):\n",
        "    text_merged, len_subject, len_details, len_text_original = merge_subject_details(raw_subject, raw_details)\n",
        "\n",
        "    text_norm = normalize_text_preserve_info(text_merged)\n",
        "\n",
        "    is_subtask = 1 if (raw_parent_uuid and pd.notna(raw_parent_uuid)) else 0\n",
        "    has_details = 1 if (len_details > 0) else 0\n",
        "    added_details = 1 if (has_details == 1 and (len_text_original > len_subject)) else 0\n",
        "\n",
        "    text_enriched = enrich_short_texts(\n",
        "        text_norm, is_subtask, len_text_original, has_details\n",
        "    )\n",
        "\n",
        "    l1_features = generate_l1_features(\n",
        "        text_enriched,\n",
        "        artifacts_bundle\n",
        "    )\n",
        "\n",
        "    extra = generate_extra_features(\n",
        "        text_enriched,\n",
        "        has_details,\n",
        "        added_details,\n",
        "        is_subtask,\n",
        "        len_subject,\n",
        "        len_details\n",
        "    )\n",
        "\n",
        "    company_features = generate_company_features_for_inference(\n",
        "        raw_company_id,\n",
        "        artifacts_bundle['company_feature_maps']\n",
        "    )\n",
        "\n",
        "    domain_features = generate_domain_features_for_inference(\n",
        "        text_enriched,\n",
        "        artifacts_bundle['domain_feature_maps']\n",
        "    )\n",
        "\n",
        "    X_meta_single = np.hstack([\n",
        "        l1_features,\n",
        "        extra,\n",
        "        domain_features,\n",
        "        company_features\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "    proba = artifacts_bundle['stacker_L2'].predict_proba(X_meta_single)[0]\n",
        "\n",
        "    return proba"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Test\n"
      ],
      "metadata": {
        "id": "oXWZrdYqb1w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _embed_texts_batch_from_artifacts(texts, artifacts_bundle, batch_size=64):\n",
        "    model_name = \"Zamza/XLM-roberta-large-ftit-emb-lr01\"\n",
        "    emb_model = AutoModel.from_pretrained(model_name)\n",
        "    emb_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device_local = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    emb_model.to(device_local)\n",
        "    emb_model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = emb_tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        enc = {k: v.to(device_local) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            out = emb_model(**enc)\n",
        "        cls_emb = out.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        all_embeddings.append(cls_emb)\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "def hdbscan_assign_batch_using_artifacts(texts, artifacts_bundle, strength_thresh=0.25, batch_size=64):\n",
        "    art_h = artifacts_bundle['hdbscan']\n",
        "    pca = art_h['pca']\n",
        "    clusterer = art_h['clusterer']\n",
        "\n",
        "    emb = _embed_texts_batch_from_artifacts(texts, artifacts_bundle, batch_size=batch_size)\n",
        "    emb_p = pca.transform(emb)\n",
        "    labels, strengths = hdbscan.approximate_predict(clusterer, emb_p)\n",
        "    labels = np.array(labels, dtype=int)\n",
        "    strengths = np.array(strengths, dtype=float)\n",
        "    in_cluster = (labels != -1) & (strengths >= strength_thresh)\n",
        "    return {'labels': labels, 'strengths': strengths, 'in_cluster': in_cluster}\n",
        "\n",
        "def experiment_skip_noise_using_artifacts(test_df, artifacts_bundle, strength_thresh=0.25, batch_size=64):\n",
        "    merged = test_df.apply(\n",
        "        lambda r: merge_subject_details(r['subject'], r['details']),\n",
        "        axis=1, result_type='expand'\n",
        "    )\n",
        "\n",
        "    test_df['text_for_hdbscan'] = merged[0]\n",
        "\n",
        "    texts = test_df['text_for_hdbscan'].apply(normalize_text_preserve_info).tolist()\n",
        "\n",
        "    assign = hdbscan_assign_batch_using_artifacts(texts, artifacts_bundle, strength_thresh=strength_thresh, batch_size=batch_size)\n",
        "\n",
        "    in_mask = assign['in_cluster']\n",
        "    n_total = len(texts)\n",
        "    n_in = int(in_mask.sum())\n",
        "    pct_in = 100.0 * n_in / n_total\n",
        "    print(f\"Всего строк: {n_total}, in-cluster: {n_in} ({pct_in:.2f}%), noise/skipped: {n_total-n_in}\")\n",
        "\n",
        "    # Predict only in-cluster\n",
        "    all_probas = []\n",
        "    all_true = []\n",
        "    idxs = np.where(in_mask)[0].tolist()\n",
        "    for i in tqdm(idxs, desc=\"Predicting in-cluster samples\"):\n",
        "        row = test_df.iloc[i]\n",
        "        proba = predict_one_pipeline(row['subject'], row['details'], row['parent_uuid'], row['company_id'], artifacts_bundle)\n",
        "        all_probas.append(proba)\n",
        "        all_true.append(int(row['label_id']))\n",
        "\n",
        "    if len(all_true) == 0:\n",
        "        print(\"Нет in-cluster примеров — ничего не считаем.\")\n",
        "        return {\"covered\": 0.0}\n",
        "\n",
        "    all_probas = np.vstack(all_probas)\n",
        "    all_true = np.array(all_true, dtype=int)\n",
        "\n",
        "    preds_top1 = np.argmax(all_probas, axis=1)\n",
        "    acc_top1 = accuracy_score(all_true, preds_top1)\n",
        "    acc_top3 = top_k_accuracy_score(all_true, all_probas, k=3)\n",
        "    acc_top5 = top_k_accuracy_score(all_true, all_probas, k=5)\n",
        "\n",
        "    print(\"\\n--- Results (only in-cluster) ---\")\n",
        "    print(f\"Covered (in-cluster) = {n_in}/{n_total} = {pct_in:.2f}%\")\n",
        "    print(f\"Top-1: {acc_top1:.5f}, Top-3: {acc_top3:.5f}, Top-5: {acc_top5:.5f}\")\n",
        "\n",
        "    cm = confusion_matrix(all_true, preds_top1, labels=range(all_probas.shape[1]))\n",
        "    return {\n",
        "        \"covered\": float(pct_in),\n",
        "        \"n_total\": n_total,\n",
        "        \"n_in\": n_in,\n",
        "        \"top1\": float(acc_top1),\n",
        "        \"top3\": float(acc_top3),\n",
        "        \"top5\": float(acc_top5),\n",
        "        \"probas\": all_probas,\n",
        "        \"y_true\": all_true,\n",
        "        \"preds_top1\": preds_top1,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "\n",
        "artifacts_bundle = load_all_prod_artifacts()\n",
        "test_df_existing_only = pd.read_csv('test_df_existing_only.csv')\n",
        "\n",
        "id2name = {\n",
        "    0: \"Аутстаффинг\", 1: \"Бухгалтерия\", 2: \"Выездные специалисты\",\n",
        "    3: \"Заправки и ремонт офисной техники\", 4: \"Напоминание\",\n",
        "    5: \"Обслуживание серверов\", 6: \"Продажи \",\n",
        "    7: \"Проектный менеджер\", 8: \"Сервисный центр\",\n",
        "    9: \"Удаленная поддержка\"\n",
        "}\n",
        "\n",
        "\n",
        "name2id = {v: k for k, v in id2name.items()}\n",
        "\n",
        "test_df_existing_only[\"label_id\"] = test_df_existing_only[\"department_name\"].map(name2id)\n",
        "\n",
        "results = experiment_skip_noise_using_artifacts(test_df_existing_only, artifacts_bundle)"
      ],
      "metadata": {
        "id": "64YugoSl8dhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "Dm2YQ32ld_2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_experiment_summary(results: dict, id2name: dict):\n",
        "    probas = results.get(\"probas\", None)\n",
        "    y_true = np.asarray(results.get(\"y_true\", []), dtype=int)\n",
        "    preds_top1 = results.get(\"preds_top1\", None)\n",
        "    if preds_top1 is None and probas is not None:\n",
        "        preds_top1 = np.argmax(probas, axis=1)\n",
        "    elif preds_top1 is None:\n",
        "        raise ValueError(\"No preds_top1 or probas found in results.\")\n",
        "\n",
        "    if probas is not None and len(probas) == len(y_true):\n",
        "        acc_top1 = accuracy_score(y_true, preds_top1)\n",
        "        acc_top3 = top_k_accuracy_score(y_true, probas, k=3)\n",
        "        acc_top5 = top_k_accuracy_score(y_true, probas, k=5)\n",
        "    else:\n",
        "        acc_top1 = accuracy_score(y_true, preds_top1)\n",
        "        acc_top3 = results.get(\"top3\", None)\n",
        "        acc_top5 = results.get(\"top5\", None)\n",
        "\n",
        "    roc_auc = roc_auc_score(\n",
        "                y_true, probas,\n",
        "                multi_class='ovr',\n",
        "                average='weighted'\n",
        "            )\n",
        "\n",
        "    n_total = results.get(\"n_total\", None)\n",
        "    n_in = results.get(\"n_in\", None)\n",
        "    covered_pct = results.get(\"covered\", None) or (100.0 * n_in / n_total if (n_total and n_in) else None)\n",
        "\n",
        "    if n_total is not None and n_in is not None:\n",
        "        print(f\"Rows total: {n_total}, in-cluster (covered): {n_in} ({covered_pct:.2f}%)\")\n",
        "    elif n_total is not None:\n",
        "        print(f\"Rows total: {n_total}\")\n",
        "    if results.get(\"covered\") is not None:\n",
        "        print(f\"in_cluster fraction: {results['covered']:.4f}\")\n",
        "\n",
        "    print(f\"\\nTop-1 Accuracy: {acc_top1:.5f}\")\n",
        "    if acc_top3 is not None:\n",
        "        print(f\"Top-3 Accuracy: {acc_top3:.5f}\")\n",
        "    if acc_top5 is not None:\n",
        "        print(f\"Top-5 Accuracy: {acc_top5:.5f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"ROC-AUC (weighted OVR): {roc_auc:.5f}\")\n",
        "\n",
        "\n",
        "    max_label = max(id2name.keys())\n",
        "    labels = list(range(max_label+1))\n",
        "    target_names = [id2name[i] for i in labels]\n",
        "\n",
        "    print(\"\\nClassification report:\\n\")\n",
        "    report_txt = classification_report(y_true, preds_top1, labels=labels, target_names=target_names, digits=4)\n",
        "    print(report_txt)\n",
        "\n",
        "    cm = confusion_matrix(y_true, preds_top1, labels=labels)\n",
        "\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    extra_df = None\n",
        "    if probas is not None:\n",
        "        top3_preds = np.argsort(probas, axis=1)[:, -3:][:, ::-1]\n",
        "        top5_preds = np.argsort(probas, axis=1)[:, -5:][:, ::-1]\n",
        "        per_class = []\n",
        "        for lab in labels:\n",
        "            idx = np.where(y_true == lab)[0]\n",
        "            if len(idx) == 0:\n",
        "                per_class.append({\n",
        "                    \"label\": lab,\n",
        "                    \"name\": id2name[lab],\n",
        "                    \"support\": 0,\n",
        "                    \"top1_recall\": None,\n",
        "                    \"in_top3_rate\": None,\n",
        "                    \"in_top5_rate\": None\n",
        "                })\n",
        "                continue\n",
        "            top1_recall = (preds_top1[idx] == lab).mean()\n",
        "            in_top3 = np.mean([lab in top3_preds[i] for i in idx])\n",
        "            in_top5 = np.mean([lab in top5_preds[i] for i in idx])\n",
        "            per_class.append({\n",
        "                \"label\": lab,\n",
        "                \"name\": id2name[lab],\n",
        "                \"support\": len(idx),\n",
        "                \"top1_recall\": float(top1_recall),\n",
        "                \"in_top3_rate\": float(in_top3),\n",
        "                \"in_top5_rate\": float(in_top5)\n",
        "            })\n",
        "        extra_df = pd.DataFrame(per_class).set_index(\"label\")\n",
        "        print(\"\\nPer-class top-k coverage (support, top1 recall, in_top3, in_top5):\")\n",
        "        print(extra_df[[\"name\",\"support\",\"top1_recall\",\"in_top3_rate\",\"in_top5_rate\"]])\n",
        "\n",
        "    return {\n",
        "        \"acc_top1\": acc_top1,\n",
        "        \"acc_top3\": acc_top3,\n",
        "        \"acc_top5\": acc_top5,\n",
        "        \"classification_report_text\": report_txt\n",
        "    }"
      ],
      "metadata": {
        "id": "dmt1Tb7SVEcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Результаты на известных компаниях без выбросов \\n\")\n",
        "summary = print_experiment_summary(results, id2name)"
      ],
      "metadata": {
        "id": "FXrmOZ3xVz9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}